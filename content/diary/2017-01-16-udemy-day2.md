+++
date = "2017-01-16T14:59:46+09:00"
title = "Udemy Day2"
draft = false
slug = "udemy-day2"
tags = ["study"]
+++

## Section 4-5
データから特徴量にしないと機械学習できませんよという話。
文字列や画像をどのように特徴量ベクトルに変換できるか。

* [Project Gutenberg](http://www.gutenberg.org/)

### Lecture 41-53

* データ取得
* 特徴量抽出
* 特徴選択
* 正規化
* 識別

欠損値、外れ値などは前処理として自分で頑張って取り除くか加工をしなければいけない。

欠損値の場合

* 取り除く
* 平均値で埋める(`Imputer`)。このときの対応として、外れ値を先に外しておくか、中央値を使う。

また次元の削減に主成分分析を使うことができる。主成分を寄与率順にいくつか選択することだけで予測率を残したまま次元数を少なくできる。

とりあえず何かしらの次元変換をする流れは線形・非線形によらず

1. 変換モデルを用意する
2. `fit()` する
3. `transform()` する

という手順になる。非線形変換で次数を増やすときは最後の手段にしよう。

標準化、正規化は次元削減よりもなによりも大事なので絶対にやろう。

ZCA白色化はDeep Learningなどによく使われる。

## Secition 6

### Lecture 54-61
2クラス問題であればconfusion matrixを作って、本当に予測が間違っていたらまずいものの個数を見極める。
さらに SciKit Learn には `classification_report` という便利なものがある。

検出したい値のrecallが大事。

`precision_recall_fscore_support` がすごく便利っぽい。

多クラス問題でも `classification_report` の値を元にどのクラスの認識率が悪いか判断できる。

### キーワード

* χ二乗検定
* 主成分分析 (PCA)
