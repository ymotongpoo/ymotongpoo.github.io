<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Coursera on depository</title>
    <link>https://ymotongpoo.github.io/tags/coursera/index.xml</link>
    <description>Recent content in Coursera on depository</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-jp</language>
    <atom:link href="https://ymotongpoo.github.io/tags/coursera/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>coursera note</title>
      <link>https://ymotongpoo.github.io/tech/2017/02/19/coursera-note/</link>
      <pubDate>Sun, 19 Feb 2017 17:22:02 +0900</pubDate>
      
      <guid>https://ymotongpoo.github.io/tech/2017/02/19/coursera-note/</guid>
      <description>

&lt;h2 id=&#34;week-3&#34;&gt;Week 3&lt;/h2&gt;

&lt;h3 id=&#34;classification-and-representation&#34;&gt;Classification and Representation&lt;/h3&gt;

&lt;p&gt;分類問題には二種類ある。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;バイナリ分類&lt;/li&gt;
&lt;li&gt;マルチクラス分類&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;線形回帰を分類問題に適用させてしまうと、ラベルの値が決まっているにも関わらず関係ない値になってしまうので、これを解決するためにロジスティクス回帰を使う。（分類問題なのに「回帰」という単語を使っているのは歴史的経緯に寄るものなので気にしなくていい。）&lt;/p&gt;

&lt;p&gt;ロジスティクス回帰では仮説関数を次のように定義する。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;h(x) = g(theta&#39; * x)
g(z) = 1 / (1 + e^(-z)) // sigmoid関数
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;sigmoid関数を使っているのは、なんか良い感じに &lt;code&gt;z=0&lt;/code&gt; で &lt;code&gt;g(z)&lt;/code&gt; がちょうど&lt;code&gt;g(z)=0.5&lt;/code&gt;を境に良い感じに線対象になるからっぽい。（要確認）&lt;/p&gt;

&lt;p&gt;これをよくよく見てみると、仮に &lt;code&gt;g(z)=0.5&lt;/code&gt; を判定基準として線を引いてみると、その実は中身の &lt;code&gt;h(x)=0&lt;/code&gt; が判定基準となる。
したがって、 &lt;code&gt;h(x)=0&lt;/code&gt; の線がうまくサンプルを分類するように &lt;code&gt;theta&lt;/code&gt; を調整してあげる必要がある。&lt;/p&gt;

&lt;h3 id=&#34;logistic-regression-model&#34;&gt;Logistic Regression Model&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;theta&lt;/code&gt; をうまく調整するのには、線形回帰でやったように、コスト関数を用意して、何らかの最適化手法（たとえば最急降下法）を使ってあげればよい。&lt;/p&gt;

&lt;p&gt;まずコスト関数は次のように定義してあげると良い。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;J(theta) = 1/m * sum(Cost(h(x), y)^2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ここで &lt;code&gt;Cost(h(x), y)&lt;/code&gt; は次のように定義する。（これは統計学の最尤法推定にもとづいているが、そういうものだと理解しておく）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Cost(h(x), y) = -log(h(x)) (y=1)
Cost(h(x), y) = -log(1 - h(x)) (y=0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;yはバイナリ分類問題のとき必ず0か1なのでこれを1つの式で表すと次のようになる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Cost(h(x), y) = -y * log(h(x)) - (1 - y)*log(1 - h(x))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これを最急降下法で解くためには各&lt;code&gt;θ&lt;/code&gt;に対して偏微分をした関数を求めて、&lt;code&gt;J&lt;/code&gt;の値が最小になるように繰り返し&lt;code&gt;θ&lt;/code&gt;を更新していく。
実はこの関数を使って&lt;code&gt;J&lt;/code&gt;の偏微分を求めると、線形回帰のときと同じような形で書ける。すなわちベクトルで書くと&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;theta := theta - alpha/m * X&#39; * (g(X*theta) - y)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;θ&lt;/code&gt; の最適化については最急降下法以外にも幾つかの手法がある。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Conjugate Descent （共役勾配法）&lt;/li&gt;
&lt;li&gt;BFGS&lt;/li&gt;
&lt;li&gt;L-BFGS&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;各々の実装は複雑なので、自前実装はせずに、すでに実装済みの効率の良いライブラリを使うべきである。（自前で &lt;code&gt;sqrt()&lt;/code&gt; や &lt;code&gt;rand()&lt;/code&gt; などを実装せずに既存の物を利用するのと同様）&lt;/p&gt;

&lt;p&gt;これらの最適化手法の利点としては、最急勾配法にあった学習率（&lt;code&gt;α&lt;/code&gt;）の調整をしなくてよいというところにある。&lt;/p&gt;

&lt;p&gt;Octaveでこれらを利用するときは &lt;code&gt;fminunc&lt;/code&gt; という関数を利用すると良い。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;function [jVal, gradient] = costFunction(theta)
  jVal = [...code to compute J(theta)...];
  gradient = [...code to compute derivative of J(theta)...];
endfunction
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;を用意してあげて、&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;options = optimset(&#39;GradObj&#39;, &#39;on&#39;, &#39;MaxIter&#39;, 100);
initialTheta = zeros(2, 1);
[optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;と呼んであげれば良い。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>coursera note</title>
      <link>https://ymotongpoo.github.io/tech/2017/02/15/coursera-note/</link>
      <pubDate>Wed, 15 Feb 2017 13:31:22 +0900</pubDate>
      
      <guid>https://ymotongpoo.github.io/tech/2017/02/15/coursera-note/</guid>
      <description>

&lt;h2 id=&#34;week-2&#34;&gt;Week 2&lt;/h2&gt;

&lt;h3 id=&#34;maultivarlate-linear-regression&#34;&gt;Maultivarlate Linear Regression&lt;/h3&gt;

&lt;p&gt;多変量線形回帰問題を考える場合は、変数をベクトルで考えてあげると良い。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;h(x) = theta^T x
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;と書ける。このとき &lt;code&gt;x0=1&lt;/code&gt; とする。内積でかけて嬉しいね。
また偏微分も基本的には全部相似形になるので簡単。&lt;/p&gt;

&lt;p&gt;再急降下法を早く収束させるために、まずできることはサンプルを正規化すること。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;feature scaling: とりあえず最大値で割って 0 =&amp;lt; x =&amp;lt; 1 にする&lt;/li&gt;
&lt;li&gt;mean normalization: 平均で引いてから、最大値と最小値の幅で割って -0.5 =&amp;lt; x =&amp;lt; 0.5にする&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;あと再急降下法の収束を決める要素として、学習率αと離散値を扱うので収束と決める条件を設定しないといけない。
それぞれ決めが難しいので、再急降下法の試行回数と &lt;code&gt;J(theta)&lt;/code&gt; の値のプロットを取って良さそうな値を決めるしかない。&lt;/p&gt;

&lt;h3 id=&#34;octave&#34;&gt;Octave&lt;/h3&gt;

&lt;h4 id=&#34;octave自体の操作&#34;&gt;Octave自体の操作&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;a&lt;/code&gt; の少数第二位まで表示。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;disp(sprintf(&#39;%0.2f), a)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3x2 の行列を宣言できる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;A = [1 2;
     3 4;
     5 6]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Aの2行目、2列目だけを抜き出す。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;A(2,:)
A(:,2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Aに行列を追加する。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;A = [A, [100; 101; 102]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Aのすべての要素を1つのベクトルにまとめる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;A(:)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2x3 のすべての要素が1と0の行列ができる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ones(2, 3)
zeros(2, 3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1x10000のランダムなベクトルを作って、ヒストグラムとして表示する。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;w = randn(1, 10000)
hist(w, 50)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4x4の単位行列を作る。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;eye(4)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;行列のサイズを調べる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;size(X)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ベクトルの長さを調べる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;length(v)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ファイルに入ったデータを読み込んで、データが格納されている変数を確認する。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;load(&#39;foo.dat&#39;)
who
whos
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;変数をクリアする。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;clear x
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;変数をファイルに保存する。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;save hello.mat v
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Aの各列での最大値を取る。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;max(A, [], 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Aの各行での最大値を取る。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;max(A, [], 2)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;plotting-data&#34;&gt;Plotting Data&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;x&lt;/code&gt;と&lt;code&gt;y&lt;/code&gt;のベクトルを使ってプロット。x軸とy軸にラベルを付ける。凡例も書く。タイトルを「graph」とする。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;plot(x, y)
xlabel(&#39;time&#39;)
ylabel(&#39;value&#39;)
legend(&#39;second&#39;, &#39;value&#39;)
title(&#39;graph&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;imagesc&lt;/code&gt; とかでヒートマップ作るのも面白い。&lt;/p&gt;

&lt;h4 id=&#34;control-statements-for-while-if-statement&#34;&gt;Control Statements; for, while, if statement&lt;/h4&gt;

&lt;p&gt;基本的に、制御文の後はカンマを付けて、最後は &lt;code&gt;end&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for i=1:10,
  if i / 2 == 0,
    disp(i);
  end;
end;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;while 1,
  disp(&#39;hello&#39;);
end;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;関数の定義は次の通り&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;function y = foo(x) % 関数の宣言

y = x^2;            % 実装
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;複数の値を返すことも出来る。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;function [y1, y2] = bar(x)

y1 = x^2;
y2 = x^3;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;addpath&lt;/code&gt;を使うと関数が定義されているファイルがあるディレクトリを追加できる。&lt;/p&gt;

&lt;p&gt;関数は&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>coursera note</title>
      <link>https://ymotongpoo.github.io/tech/2017/02/14/coursera-note/</link>
      <pubDate>Tue, 14 Feb 2017 12:50:46 +0900</pubDate>
      
      <guid>https://ymotongpoo.github.io/tech/2017/02/14/coursera-note/</guid>
      <description>

&lt;h2 id=&#34;week-1&#34;&gt;Week 1&lt;/h2&gt;

&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;2つのタイプの機械学習がある。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;regression （回帰）: 連続値&lt;/li&gt;
&lt;li&gt;classification （分類）: ラベリング&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ほかにもこういう分け方もある。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;supervised （教師あり）: サンプルに正解がすでに存在している&lt;/li&gt;
&lt;li&gt;unsupervised （教師なし）: サンプルはただ存在している&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;model-and-cost-function&#34;&gt;Model and Cost Function&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;x: input (サンプル)&lt;/li&gt;
&lt;li&gt;y: output (結果)&lt;/li&gt;
&lt;li&gt;m: サンプル数&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;たとえば教師ありの回帰問題を考える。仮説関数 (hypothesis)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;h(x) = theta0 + theta1 x
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これは線形回帰かつ単回帰（線形に表現され、変数が1個）な場合の仮説関数。
この関数をサンプルから求めるために目的関数（コスト関数）を用意する。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;J(theta0, theta1) = 1/2m sum(( h(x) - y )^2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これを最小にする&lt;code&gt;theta0&lt;/code&gt;と&lt;code&gt;theta1&lt;/code&gt;を自動で求めていきたいということになる。&lt;/p&gt;

&lt;h3 id=&#34;parameter-learning&#34;&gt;Parameter Learning&lt;/h3&gt;

&lt;p&gt;いろいろな方法があるけれど、再急降下法でやっていくのが一般的。
各パラメータで偏微分して、各パラメータを減らしていく。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;theta(j) := theta(j) - alpha * grad(J, theta(j))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;プログラムで書くときは値の更新が各変数で同時になるように注意。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;alpha&lt;/code&gt; は学習率と呼ばれる正の値。いい具合に決めてやらないと発散しちゃうので注意。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>