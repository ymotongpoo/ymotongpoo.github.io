<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ml on depository</title>
    <link>https://ymotongpoo.github.io/tags/ml/index.xml</link>
    <description>Recent content in Ml on depository</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-jp</language>
    <atom:link href="https://ymotongpoo.github.io/tags/ml/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>coursera note</title>
      <link>https://ymotongpoo.github.io/tech/2017/02/14/coursera-note/</link>
      <pubDate>Tue, 14 Feb 2017 12:50:46 +0900</pubDate>
      
      <guid>https://ymotongpoo.github.io/tech/2017/02/14/coursera-note/</guid>
      <description>

&lt;h2 id=&#34;week-1&#34;&gt;Week 1&lt;/h2&gt;

&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;2つのタイプの機械学習がある。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;regression （回帰）: 連続値&lt;/li&gt;
&lt;li&gt;classification （分類）: ラベリング&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ほかにもこういう分け方もある。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;supervised （教師あり）: サンプルに正解がすでに存在している&lt;/li&gt;
&lt;li&gt;unsupervised （教師なし）: サンプルはただ存在している&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;model-and-cost-function&#34;&gt;Model and Cost Function&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;x: input (サンプル)&lt;/li&gt;
&lt;li&gt;y: output (結果)&lt;/li&gt;
&lt;li&gt;m: サンプル数&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;たとえば教師ありの回帰問題を考える。仮説関数 (hypothesis)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;h(x) = theta0 + theta1 x
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これは線形回帰かつ単回帰（線形に表現され、変数が1個）な場合の仮説関数。
この関数をサンプルから求めるために目的関数（コスト関数）を用意する。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;J(theta0, theta1) = 1/2m sum(( h(x) - y )^2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これを最小にする&lt;code&gt;theta0&lt;/code&gt;と&lt;code&gt;theta1&lt;/code&gt;を自動で求めていきたいということになる。&lt;/p&gt;

&lt;h3 id=&#34;parameter-learning&#34;&gt;Parameter Learning&lt;/h3&gt;

&lt;p&gt;いろいろな方法があるけれど、再急降下法でやっていくのが一般的。
各パラメータで偏微分して、各パラメータを減らしていく。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;theta(j) := theta(j) - alpha * grad(J, theta(j))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;プログラムで書くときは値の更新が各変数で同時になるように注意。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;alpha&lt;/code&gt; は学習率と呼ばれる正の値。いい具合に決めてやらないと発散しちゃうので注意。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Udemy Day1</title>
      <link>https://ymotongpoo.github.io/diary/2017/01/12/udemy-day1/</link>
      <pubDate>Thu, 12 Jan 2017 14:04:30 +0900</pubDate>
      
      <guid>https://ymotongpoo.github.io/diary/2017/01/12/udemy-day1/</guid>
      <description>

&lt;h2 id=&#34;section-3&#34;&gt;Section 3&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLDVKrmVEl_Wt0WoaQqDfRVt9EK1lQjZu5&#34;&gt;YouTubeのscikit-learnチュートリアル集（英語）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v&#34;&gt;Machine Learning with Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.kamishima.net/mlmpyja/&#34;&gt;機械学習の Python との出会い Machine Learning Meets Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.slideshare.net/yasutomo57jp/pythondeep-learning-60544586&#34;&gt;Pythonによる機械学習入門〜基礎からDeep Learningまで〜（電子情報通信学会総合大会2016）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.slideshare.net/yasutomo57jp/python-svmdeep-learning&#34;&gt;Pythonによる機械学習入門 ～SVMからDeep Learningまで～（SSII2016）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.co.jp/dp/4844380605/&#34;&gt;Python機械学習プログラミング 達人データサイエンティストによる理論と実践, インプレス, 2016&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.co.jp/dp/4873116988/&#34;&gt;実践 機械学習システム, オライリージャパン, 2014&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;lecture-29-33&#34;&gt;Lecture 29-33&lt;/h2&gt;

&lt;h3 id=&#34;scikit-learnでの大まかな手順&#34;&gt;scikit-learnでの大まかな手順&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;データセットを用意する&lt;/li&gt;
&lt;li&gt;学習用とテスト用にデータセットを分ける&lt;/li&gt;
&lt;li&gt;識別器を用意する&lt;/li&gt;
&lt;li&gt;学習させる&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;データセットを分ける場合はランダムに分けるように気をつけて。また分割を何度も行って、正解率の平均と標準偏差を出しておこう。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Cross Validation (交差確認)

&lt;ul&gt;
&lt;li&gt;K-fold CV (K=3, 5, 10)&lt;/li&gt;
&lt;li&gt;Leave one out&lt;/li&gt;
&lt;li&gt;Leave group out&lt;/li&gt;
&lt;li&gt;Hold out&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Stratified (層化)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;識別器はScikit Learnにいろいろプリセットで用意されていて楽ちん。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn import linear_model
cls = linear_model.LogisticRegression()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;学習させるのは識別器の &lt;code&gt;fit()&lt;/code&gt; メソッドを使うだけ。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cls.fit(x_train, y_train)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;サンプル数によってやり方が変わってくる。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;N ~ 10 : 機械学習する意味ない&lt;/li&gt;
&lt;li&gt;N ~ 100 : Leave one out CVで可能かも。でもデータは増やしたい。&lt;/li&gt;
&lt;li&gt;N ~ 1000 : まともになってくる。10-fold CVで可能。&lt;/li&gt;
&lt;li&gt;N ~ 10000 : 良い性能が期待できる。K-fold CV (K &amp;lt; 10)&lt;/li&gt;
&lt;li&gt;N ~ 100000 : 実応用。Hold-out以外は無理。かなり工夫が必要。&lt;/li&gt;
&lt;li&gt;N &amp;gt; 100000 : 最先端&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;キーワードを拾っていく&#34;&gt;キーワードを拾っていく&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;ロジスティクス回帰&lt;/li&gt;
&lt;li&gt;Cross Validation&lt;/li&gt;
&lt;li&gt;Hold out&lt;/li&gt;
&lt;li&gt;Stratified&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>